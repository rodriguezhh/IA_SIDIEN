{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IA_secuencial.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOADzH709SRFjFYtbxVCQs2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rodriguezhh/IA_SIDIEN/blob/main/IA_secuencial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONSTRUYENDO UN MODELO SECUENCIAL\n"
      ],
      "metadata": {
        "id": "AKykCQvqm504"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset mediante DRIVE\n"
      ],
      "metadata": {
        "id": "mG9yQ9TknKFx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16Mhlwxemkuq",
        "outputId": "431a2a1a-6fa9-44ed-c776-129d4cddd9ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manipulación del Dataset\n"
      ],
      "metadata": {
        "id": "sMFCAu7_nVEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "IMAGE_SIZE = 256\n",
        "BATCH_SIZE = 64\n",
        "CHANELS = 3\n",
        "EPOCHS = 20\n",
        "\n",
        "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/drive/MyDrive/Universidad/Tesis/Dataset/color\",\n",
        "    shuffle=True,\n",
        "    image_size = (IMAGE_SIZE,IMAGE_SIZE),\n",
        "    batch_size = BATCH_SIZE\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Po7BO6wDmv2X",
        "outputId": "c450097f-0e30-46b6-a9cf-fe97b409ecaa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8070 files belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtención de las clases y visualización"
      ],
      "metadata": {
        "id": "eiCHk4Nmnlu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = dataset.class_names\n",
        "class_names"
      ],
      "metadata": {
        "id": "mDHtuVN9m5Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se define los datos de entrenamiento, de validación y de testeo, mediante una Función.\n"
      ],
      "metadata": {
        "id": "pwA4OyfpnwC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset_partitions_tf(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
        "\n",
        "    ds_size = len(ds)\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(shuffle_size, seed=12)\n",
        "\n",
        "    train_size = int(train_split * ds_size)\n",
        "    val_size = int(val_split * ds_size)\n",
        "\n",
        "    train_ds = ds.take(train_size)\n",
        "\n",
        "    val_ds = ds.skip(train_size).take(val_size)\n",
        "    test_ds = ds.skip(train_size).skip(val_size)\n",
        "    \n",
        "    return train_ds, val_ds, test_ds"
      ],
      "metadata": {
        "id": "bjJ6R2a3mxK-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se guardan los datos\n"
      ],
      "metadata": {
        "id": "cph6mdpqohYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, val_ds, test_ds = get_dataset_partitions_tf(dataset)"
      ],
      "metadata": {
        "id": "gjA5xvhDmx-e"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_ds), len(val_ds), len(test_ds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQs5UC6Cmz_H",
        "outputId": "30744cda-b989-45ca-f1b0-f7c495ee9e5a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101 12 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resize and rescale"
      ],
      "metadata": {
        "id": "WmX2HY-3pJ5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resize_and_rescale = tf.keras.Sequential([\n",
        "    layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    layers.experimental.preprocessing.Rescaling(1.0/255)\n",
        "])"
      ],
      "metadata": {
        "id": "rV5iTGl_pJEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data augmentation"
      ],
      "metadata": {
        "id": "Dox_6jpVpKkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.2),\n",
        "     ]\n",
        ")"
      ],
      "metadata": {
        "id": "Q9qfZYCdpLCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construyendo el modelo \n"
      ],
      "metadata": {
        "id": "YLmuwbbRpag8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANELS)\n",
        "\n",
        "model = models.Sequential([\n",
        "    resize_and_rescale,\n",
        "    data_augmentation, \n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape = input_shape),\n",
        "    layers.MaxPool2D((2, 2)),\n",
        "    layers.Conv2D(16, (3,3), activation='relu'),\n",
        "    layers.MaxPool2D((2, 2)),\n",
        "    layers.Conv2D(8, (3,3), activation='relu'),\n",
        "    layers.MaxPool2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(10, activation='softmax'),\n",
        "])\n",
        "\n",
        "model.build(input_shape=input_shape)"
      ],
      "metadata": {
        "id": "SLSA-Clrpc6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "yMeytPcppntB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"rmsprop\",\n",
        " loss=\"sparse_categorical_crossentropy\",\n",
        " metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "5cLGlzubpp89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_ds,\n",
        "    batch_size = BATCH_SIZE\n",
        ")"
      ],
      "metadata": {
        "id": "2TqE5pwIptbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "keYg3G1Crg-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images_batch, labels_batch in test_ds.take(1):\n",
        "    \n",
        "    first_image = images_batch[0].numpy().astype('uint8')\n",
        "    first_label = labels_batch[0].numpy()\n",
        "\n",
        "    print('first image to predict')\n",
        "    plt.imshow(first_image)\n",
        "    print('actual label', class_names[first_label])\n",
        "\n",
        "    batch_prediction = model.predict(images_batch)\n",
        "    print(\"predicted label: \", class_names[np.argmax(batch_prediction[0])])"
      ],
      "metadata": {
        "id": "LOFEaQqKsLu-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}